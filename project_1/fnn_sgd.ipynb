{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN:\n",
    "    def __init__(self, input_size, input_layer_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        # self.W_ii = np.random.rand(input_layer_size, input_size) / 100000\n",
    "        # self.B_i = np.random.rand(input_layer_size, 1)/ 100000\n",
    "        # self.W_ih = np.random.rand(hidden_size, input_layer_size) / 100000\n",
    "        # self.B_h = np.random.rand(hidden_size, 1)/ 100000\n",
    "        # self.W_ho = np.random.rand(output_size, hidden_size) / 100000\n",
    "        # self.B_o = np.random.rand(output_size, 1)/ 100000\n",
    "\n",
    "        self.W_ii = np.zeros((input_layer_size, input_size))\n",
    "        self.B_i = np.zeros((input_layer_size, 1))\n",
    "        self.W_ih = np.zeros((hidden_size, input_layer_size))\n",
    "        self.B_h = np.zeros((hidden_size, 1))\n",
    "        self.W_ho = np.zeros((output_size, hidden_size))\n",
    "        self.B_o = np.zeros((output_size, 1))\n",
    "\n",
    "    def feedforward(self, input_data):\n",
    "        input_input = np.dot(self.W_ii, input_data.T) + self.B_i\n",
    "        input_output = sigmoid(input_input)\n",
    "        hidden_input = np.dot(self.W_ih, input_output) + self.B_h\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output_hidden = np.dot(self.W_ho, hidden_output) + self.B_o\n",
    "        output = sigmoid(output_hidden).T\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as ff:\n",
    "    training_data, validation_data, test_data = pickle.load(\n",
    "        ff, encoding='bytes')\n",
    "print(training_data[0].shape)\n",
    "print(validation_data[0].shape)\n",
    "print(test_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def to_onehot(labels):\n",
    "    one_hot_labels = np.zeros((len(labels), 10))\n",
    "    for i, label in enumerate(labels):\n",
    "        one_hot_labels[i, label] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " ## TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def backpropagation(model, input_data, target_output, learning_rate):\n",
    "    input_input = np.dot(model.W_ii, input_data.T) + model.B_i\n",
    "    input_output = sigmoid(input_input)  # 50 bat\n",
    "    hidden_input = np.dot(model.W_ih, input_output) + model.B_h\n",
    "    hidden_output = sigmoid(hidden_input)  # 30 bat\n",
    "    output_hidden = np.dot(model.W_ho, hidden_output) + model.B_o\n",
    "    output = sigmoid(output_hidden).T  # bat 10\n",
    "\n",
    "    # Calculate the error at the output layer\n",
    "    loss = output - target_output\n",
    "    output_delta = loss * sigmoid_derivative(output)  # bat 10\n",
    "\n",
    "    # Calculate the error at the hidden layer\n",
    "    hidden_error = output_delta.dot(model.W_ho)  # bat 30\n",
    "    hidden_delta = hidden_error * sigmoid_derivative(hidden_output).T  # bat 30\n",
    "\n",
    "    # Calculate the error at the input layer\n",
    "    input_error = hidden_delta.dot(model.W_ih)  # bat 50\n",
    "    input_delta = input_error * sigmoid_derivative(input_output.T)  # bat 50\n",
    "\n",
    "    # Update W and Bes for each layer\n",
    "    model.W_ho -= (hidden_output.dot(output_delta)).T * learning_rate / len(input_delta)\n",
    "    model.B_o -= np.sum(output_delta, axis=0, keepdims=True).T * learning_rate / len(input_delta)\n",
    "    model.W_ih -= (input_output.dot(hidden_delta)).T * learning_rate / len(input_delta)\n",
    "    model.B_h -= np.sum(hidden_delta, axis=0, keepdims=True).T * learning_rate / len(input_delta)\n",
    "    model.W_ii -= input_delta.T.dot(input_data) * learning_rate / len(input_delta)\n",
    "    model.B_i -= np.sum(input_delta, axis=0, keepdims=True).T * learning_rate / len(input_delta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_loss(model, inputs, labels):\n",
    "    output = model.feedforward(inputs)\n",
    "    loss = 0.5 * np.linalg.norm(output - labels, axis=1) ** 2\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, inputs, labels):\n",
    "    outputs = model.feedforward(inputs)\n",
    "    correct_predictions = np.sum(np.argmax(outputs, axis=1) == np.argmax(labels, axis=1))\n",
    "    return correct_predictions / len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(model, data, mini_batch_size, learning_rate, epochs):\n",
    "    n = len(data)\n",
    "    inputs, labels = data[0], data[1]\n",
    "    labels = to_onehot(labels)\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        mini_batches = [(inputs[k:k + mini_batch_size], labels[k:k + mini_batch_size]) for k in\n",
    "                        range(0, n, mini_batch_size)]\n",
    "        for x, y in mini_batches:\n",
    "            backpropagation(model, x, y, learning_rate)\n",
    "\n",
    "        # Print learning success per epoch\n",
    "        accuracy = evaluate(model, inputs, labels)\n",
    "        loss_history.append(quadratic_loss(model, inputs, labels))\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Accuracy {accuracy * 100:.2f}%, Loss {loss_history[-1]:.4f}\")\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.plot(range(epochs), loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "input_layer_size = 785\n",
    "hidden_size = 31\n",
    "output_size = 10\n",
    "learning_rate = 0.1\n",
    "mini_batch_size = 32\n",
    "epochs = 1000\n",
    "model = FeedForwardNN(input_size, input_layer_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: Accuracy 11.36%, Loss 1.1649\n",
      "Epoch 2/1000: Accuracy 11.36%, Loss 1.0877\n",
      "Epoch 3/1000: Accuracy 11.36%, Loss 1.0159\n",
      "Epoch 4/1000: Accuracy 11.36%, Loss 0.9482\n",
      "Epoch 5/1000: Accuracy 11.36%, Loss 0.8846\n",
      "Epoch 6/1000: Accuracy 11.36%, Loss 0.8253\n",
      "Epoch 7/1000: Accuracy 11.36%, Loss 0.7713\n",
      "Epoch 8/1000: Accuracy 11.36%, Loss 0.7231\n",
      "Epoch 9/1000: Accuracy 11.36%, Loss 0.6811\n",
      "Epoch 10/1000: Accuracy 11.36%, Loss 0.6452\n",
      "Epoch 11/1000: Accuracy 11.36%, Loss 0.6150\n",
      "Epoch 12/1000: Accuracy 11.36%, Loss 0.5898\n",
      "Epoch 13/1000: Accuracy 11.36%, Loss 0.5690\n",
      "Epoch 14/1000: Accuracy 11.36%, Loss 0.5518\n",
      "Epoch 15/1000: Accuracy 11.36%, Loss 0.5375\n",
      "Epoch 16/1000: Accuracy 11.36%, Loss 0.5258\n",
      "Epoch 17/1000: Accuracy 11.36%, Loss 0.5160\n",
      "Epoch 18/1000: Accuracy 11.36%, Loss 0.5078\n",
      "Epoch 19/1000: Accuracy 11.36%, Loss 0.5009\n",
      "Epoch 20/1000: Accuracy 11.36%, Loss 0.4951\n",
      "Epoch 21/1000: Accuracy 11.36%, Loss 0.4902\n",
      "Epoch 22/1000: Accuracy 11.36%, Loss 0.4860\n",
      "Epoch 23/1000: Accuracy 11.36%, Loss 0.4824\n",
      "Epoch 24/1000: Accuracy 11.36%, Loss 0.4793\n",
      "Epoch 25/1000: Accuracy 11.36%, Loss 0.4766\n",
      "Epoch 26/1000: Accuracy 11.36%, Loss 0.4742\n",
      "Epoch 27/1000: Accuracy 11.36%, Loss 0.4722\n",
      "Epoch 28/1000: Accuracy 11.36%, Loss 0.4704\n",
      "Epoch 29/1000: Accuracy 11.36%, Loss 0.4688\n",
      "Epoch 30/1000: Accuracy 11.36%, Loss 0.4674\n",
      "Epoch 31/1000: Accuracy 11.36%, Loss 0.4662\n",
      "Epoch 32/1000: Accuracy 11.36%, Loss 0.4651\n",
      "Epoch 33/1000: Accuracy 11.36%, Loss 0.4641\n",
      "Epoch 34/1000: Accuracy 11.36%, Loss 0.4632\n",
      "Epoch 35/1000: Accuracy 11.36%, Loss 0.4625\n",
      "Epoch 36/1000: Accuracy 11.36%, Loss 0.4618\n",
      "Epoch 37/1000: Accuracy 11.36%, Loss 0.4611\n",
      "Epoch 38/1000: Accuracy 11.36%, Loss 0.4606\n",
      "Epoch 39/1000: Accuracy 11.36%, Loss 0.4601\n",
      "Epoch 40/1000: Accuracy 11.36%, Loss 0.4596\n",
      "Epoch 41/1000: Accuracy 11.36%, Loss 0.4592\n",
      "Epoch 42/1000: Accuracy 11.36%, Loss 0.4588\n",
      "Epoch 43/1000: Accuracy 11.36%, Loss 0.4585\n"
     ]
    }
   ],
   "source": [
    "SGD(model, training_data, mini_batch_size, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data:  Accuracy 11.35%, Loss 0.4574\n"
     ]
    }
   ],
   "source": [
    "test_input, test_labels = test_data[0], to_onehot(test_data[1])\n",
    "test_accuracy = evaluate(model, test_input, test_labels)\n",
    "test_loss = quadratic_loss(model, test_input, test_labels)\n",
    "print(f\"test data:  Accuracy {test_accuracy * 100:.2f}%, Loss {test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
